# -*- coding: utf-8 -*-
"""SIC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jDulrKsRh1Dz_I5fSgyErn9Q0ifwIHEk
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

file = open("ignis-spanish-150.txt", "r", encoding = "latin-1")
text = file.read()
file.close()

body = text.lower().split("\n")

F_body = list(set(body))

tokens = [list(password) for password in F_body]

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(char_level = True)
tokenizer.fit_on_texts("".join(password) for password in tokens)

input_sequences = []

for word in tokens:
  token_list = tokenizer.texts_to_sequences([word])[0]
  for i in range(1, len(token_list)):
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)

input_sequences = np.array(pad_sequences(input_sequences, padding = "pre", maxlen=20))
input_sequences[:5]

x = input_sequences[:,:-1]
y = input_sequences[:,-1]

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(151, 64, input_length = 20),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers .Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(128, activation = "relu"),
    tf.keras.layers.Dense(151, activation = "softmax")
])

model.compile(loss = "sparse_categorical_crossentropy",
              optimizer = tf.keras.optimizers.Adam(0.0001),
              metrics = ["accuracy"])

#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 *10**(epoch/20))

#earlyStoping = tf.keras.callbacks.EarlyStopping(monitor = "loss", patience = 20, restore_best_weights = True)
history = model.fit(x, y, epochs = 150)

real_password = input("contraseña real:")
try_password = input("caracteres a probar:")

gen_passwords = []

test_text = tokenizer.texts_to_sequences([real_password])[0]
padded_test = tf.keras.preprocessing.sequence.pad_sequences([test_text], maxlen=12, padding="pre")

text = tokenizer.texts_to_sequences([try_password])[0]
padded_text = tf.keras.preprocessing.sequence.pad_sequences([text], maxlen=12, padding="pre")

first_pred_chars = None # Variable para almacenar la primera lista pred_chars
unique_gen_text = []

def decode_sequence(sequence, tokenizer):
    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))
    flat_sequence = np.ravel(sequence).tolist() # Aplanar cualquier arreglo de numpy a una lista de enteros
    return "".join([reverse_word_map.get(i, "") for i in flat_sequence])

def crack_password(model, test_text, padded_text, text, first_run=True):
    global first_pred_chars # Utiliza la variable global para almacenar la primera lista pred_chars

    preds = model.predict(padded_text, verbose=0)[0]
    pred_chars = np.argsort(preds)

    if first_run:
        first_pred_chars = pred_chars # Almacena la primera lista pred_chars
        print(f"Primera lista char_n: {decode_sequence([first_pred_chars], tokenizer)}")

    gen_chars = decode_sequence([pred_chars], tokenizer)
    list_chars = list(gen_chars)
    print(f"char_n:{list_chars}")

    for char in pred_chars:
        new_text = text + [char]
        gen_text = decode_sequence([new_text], tokenizer)
        if gen_text not in gen_passwords:
            gen_passwords.append(gen_text)
        pad_preds = tf.keras.preprocessing.sequence.pad_sequences([new_text], maxlen=12, padding="pre")

    if len(new_text) > len(test_text):
        return None

    if new_text != test_text:
        return crack_password(model, test_text, pad_preds, new_text, first_run=False)
    else:
        return new_text

final_password = crack_password(model, test_text, padded_text, text)
print(final_password)

generated_password = decode_sequence([final_password], tokenizer)

print(f"Predicción final: {final_password}, contraseña original: {test_text}, contraseña generada: {generated_password}")

# Generar predicciones adicionales con la primera lista pred_chars
if first_pred_chars is not None:
    print("Predicciones adicionales con la primera lista pred_chars:")
    for char in first_pred_chars:
        prob_text = text + [char]
        gen_text = decode_sequence([prob_text], tokenizer)
        pad_preds = tf.keras.preprocessing.sequence.pad_sequences([prob_text], maxlen=12, padding="pre")

        if gen_text not in unique_gen_text:
            unique_gen_text.append(gen_text)  # Añadir la secuencia única al conjunto

        if len(prob_text) >= len(test_text):
           crack_password(model, test_text, pad_preds, prob_text, first_run=False)

print(f"Generado: {unique_gen_text}")

for unique_gen_text in unique_gen_text:
  alt_text = tokenizer.texts_to_sequences([unique_gen_text])[0]
  alt_padded_text = tf.keras.preprocessing.sequence.pad_sequences([alt_text], maxlen=12, padding="pre")
  alt_passwords = crack_password(model, test_text, alt_padded_text, alt_text, first_run=False)
  print(alt_passwords)
  alt_gen_password = decode_sequence([alt_passwords], tokenizer)
  print(f"Generado: {alt_gen_password}")